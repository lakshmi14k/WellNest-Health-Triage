{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b568f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Importing Basic Packgaes\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7caa6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# WellNest ML Training Pipeline - DIABETES DOMAIN\n",
      "################################################################################\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Reading CSV file: C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv\n",
      "âœ“ Loaded 86,641 rows with 32 columns\n",
      "\n",
      "Final dataset: 86,641 rows\n",
      "\n",
      "Target variable distribution:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            84618\n",
      "needs_attention     1350\n",
      "urgent               673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportions:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            0.976651\n",
      "needs_attention    0.015582\n",
      "urgent             0.007768\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE PREPARATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Selected 31 feature columns\n",
      "âœ“ Target column: GLUCOSE_URGENCY_LEVEL\n",
      "\n",
      "âœ“ Categorical features: 13\n",
      "âœ“ Numerical features: 8\n",
      "\n",
      "âœ“ Handling missing values...\n",
      "\n",
      "âœ“ Encoding categorical variables...\n",
      "  - GENDER: 3 unique values\n",
      "  - SMOKING_HISTORY: 6 unique values\n",
      "  - DBT_LOADED_AT: 1 unique values\n",
      "  - DIABETES_STAGE: 4 unique values\n",
      "  - GLUCOSE_CONTROL_STATUS: 5 unique values\n",
      "  - GLUCOSE_HBA1C_CONCORDANCE: 4 unique values\n",
      "  - BMI_CATEGORY: 7 unique values\n",
      "  - SMOKING_STATUS_CLEAN: 5 unique values\n",
      "  - HYPERGLYCEMIA_URGENCY: 3 unique values\n",
      "  - HYPOGLYCEMIA_URGENCY: 1 unique values\n",
      "  - WEIGHT_MANAGEMENT_PRIORITY: 5 unique values\n",
      "  - SMOKING_CESSATION_PRIORITY: 5 unique values\n",
      "  - AGE_RISK_CATEGORY: 3 unique values\n",
      "\n",
      "âœ“ Scaling numerical features...\n",
      "\n",
      "âœ“ Encoding target variable...\n",
      "  - Target classes: ['needs_attention' 'routine' 'urgent']\n",
      "\n",
      "âœ“ Final feature matrix shape: (86641, 31)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: SPLITTING DATA\n",
      "================================================================================\n",
      "\n",
      "âœ“ Train set: 69,312 samples (80.0%)\n",
      "âœ“ Test set: 17,329 samples (20.0%)\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: XGBoost\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (this may take a few minutes)...\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "\n",
      "âœ“ Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost Test Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16924\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17329\n",
      "   macro avg       1.00      1.00      1.00     17329\n",
      "weighted avg       1.00      1.00      1.00     17329\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16924     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16924\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_xgboost_feature_importance.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features:\n",
      "  1. HYPERGLYCEMIA_URGENCY: 0.5525\n",
      "  2. BLOOD_GLUCOSE_LEVEL: 0.4475\n",
      "  3. HAS_PREMATURE_DISEASE: 0.0000\n",
      "  4. IS_OBESE: 0.0000\n",
      "  5. AGE: 0.0000\n",
      "  6. HAS_HYPERTENSION: 0.0000\n",
      "  7. HAS_HEART_DISEASE: 0.0000\n",
      "  8. SMOKING_HISTORY: 0.0000\n",
      "  9. BMI: 0.0000\n",
      "  10. HBA1C_LEVEL: 0.0000\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 2: Random Forest\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (this may take a few minutes)...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "âœ“ Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Test Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16924\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17329\n",
      "   macro avg       1.00      1.00      1.00     17329\n",
      "weighted avg       1.00      1.00      1.00     17329\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16924     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16924\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_random_forest_feature_importance.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features:\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.4244\n",
      "  2. HYPERGLYCEMIA_URGENCY: 0.3305\n",
      "  3. GLUCOSE_CONTROL_STATUS: 0.1237\n",
      "  4. HAS_DIABETES: 0.0490\n",
      "  5. CARDIOMETABOLIC_DISEASE_COUNT: 0.0179\n",
      "  6. HBA1C_LEVEL: 0.0138\n",
      "  7. GLUCOSE_HBA1C_CONCORDANCE: 0.0106\n",
      "  8. CARDIOVASCULAR_RISK_SCORE: 0.0059\n",
      "  9. DIABETES_COMPLICATION_RISK_SCORE: 0.0037\n",
      "  10. HAS_HYPERTENSION: 0.0036\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 3: SVM\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (this may take a few minutes)...\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "âœ“ Best parameters: {'C': 0.1, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SVM Test Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16924\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17329\n",
      "   macro avg       1.00      1.00      1.00     17329\n",
      "weighted avg       1.00      1.00      1.00     17329\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16924     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16924\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "================================================================================\n",
      "STEP 5: SAVING MODELS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved xgboost to: diabetes_xgboost_20251024_171339.joblib\n",
      "\n",
      "âœ“ Saved random_forest to: diabetes_random_forest_20251024_171339.joblib\n",
      "\n",
      "âœ“ Saved svm to: diabetes_svm_20251024_171339.joblib\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE! ðŸŽ‰\n",
      "================================================================================\n",
      "\n",
      "âœ“ Models trained and saved:\n",
      "  - diabetes_xgboost_20251024_171339.joblib\n",
      "  - diabetes_random_forest_20251024_171339.joblib\n",
      "  - diabetes_svm_20251024_171339.joblib\n",
      "\n",
      "âœ“ Feature importance plots generated:\n",
      "  - diabetes_xgboost_feature_importance.png\n",
      "  - diabetes_random_forest_feature_importance.png\n",
      "\n",
      "ðŸ’¡ Next steps:\n",
      "  1. Review the classification reports above\n",
      "  2. Check feature importance plots\n",
      "  3. Choose the best performing model\n",
      "  4. Use the saved .joblib file for predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Modeling Diabetes DataSet\n",
    "80-20\n",
    "\"\"\"\n",
    "\n",
    "# Your CSV file name\n",
    "CSV_FILE =r'C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv'  # â† Change this to your file name\n",
    "\n",
    "# Your target column name (the urgency level you want to predict)\n",
    "TARGET_COLUMN = 'GLUCOSE_URGENCY_LEVEL'  # â† Change if your column name is different\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_diabetes_data(csv_file, target_col):\n",
    "    \"\"\"Load diabetes feature data from CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nReading CSV file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"\\nâŒ ERROR: Target column '{target_col}' not found!\")\n",
    "        print(f\"Available columns: {df.columns.tolist()[:10]}...\")\n",
    "        return None\n",
    "    \n",
    "    # Remove rows with missing target values\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    \n",
    "    if removed_rows > 0:\n",
    "        print(f\"âœ“ Removed {removed_rows:,} rows with missing target values\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df):,} rows\")\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget proportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: PREPARE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare features for ML training:\n",
    "    - Remove non-feature columns\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables\n",
    "    - Scale numerical features\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: FEATURE PREPARATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Columns to exclude from features (add any others specific to your data)\n",
    "    exclude_cols = [\n",
    "        target_col,\n",
    "        'patient_id', 'record_id', 'id', 'index',\n",
    "        'created_at', 'updated_at', 'timestamp',\n",
    "        # Remove Tier 3 conversational flags (if they exist)\n",
    "        'should_ask_diabetic_symptoms',\n",
    "        'should_ask_cardiovascular_symptoms',\n",
    "        'should_ask_diet_habits',\n",
    "        'should_ask_physical_activity',\n",
    "        'should_ask_medication_adherence',\n",
    "        'needs_specialist_referral_flag',\n",
    "        'priority_education_topics'\n",
    "    ]\n",
    "    \n",
    "    # Get feature columns (exclude non-features)\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in exclude_cols and col in df.columns]\n",
    "    \n",
    "    print(f\"\\nâœ“ Selected {len(feature_cols)} feature columns\")\n",
    "    print(f\"âœ“ Target column: {target_col}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ“ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"âœ“ Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nâœ“ Handling missing values...\")\n",
    "    \n",
    "    # For numerical columns: fill with median\n",
    "    for col in numerical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with median\")\n",
    "    \n",
    "    # For categorical columns: fill with mode or 'unknown'\n",
    "    for col in categorical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_value = X[col].mode()[0] if not X[col].mode().empty else 'unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\nâœ“ Encoding categorical variables...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  - {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nâœ“ Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(\"\\nâœ“ Encoding target variable...\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    print(f\"  - Target classes: {target_encoder.classes_}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoders, scaler, target_encoder\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: TRAIN MODELS\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train XGBoost model with hyperparameter tuning\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 1: XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Hyperparameter grid (simplified for faster training)\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200],\n",
    "        'min_child_weight': [1, 3],\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (this may take a few minutes)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, \n",
    "        param_grid, \n",
    "        cv=3,  # 3-fold cross-validation\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"XGBoost Test Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train Random Forest model with hyperparameter tuning\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 2: Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (this may take a few minutes)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf, \n",
    "        param_grid, \n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Random Forest Test Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train SVM model with hyperparameter tuning\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 3: SVM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svm = SVC(random_state=42, probability=True)\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (this may take a few minutes)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        svm, \n",
    "        param_grid, \n",
    "        cv=3,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SVM Test Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):\n",
    "        print(f\"  Class {i}: Precision={p:.4f}, Recall={r:.4f}, F1={f:.4f}, Support={s}\")\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"Top 20 Feature Importance - {model_name}\")\n",
    "        plt.bar(range(20), importances[indices])\n",
    "        plt.xticks(range(20), [feature_names[i] for i in indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"diabetes_{model_name.lower().replace(' ', '_')}_feature_importance.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved feature importance plot: {filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Top 10 Most Important Features:\")\n",
    "        for i in range(min(10, len(indices))):\n",
    "            print(f\"  {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# WellNest ML Training Pipeline - DIABETES DOMAIN\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # STEP 1: Load data\n",
    "    df = load_diabetes_data(CSV_FILE, TARGET_COLUMN)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nâŒ Failed to load data. Please check your CSV file and target column name.\")\n",
    "        return\n",
    "    \n",
    "    # STEP 2: Prepare features\n",
    "    X, y, label_encoders, scaler, target_encoder = prepare_features(df, TARGET_COLUMN)\n",
    "    \n",
    "    # STEP 3: Split data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: SPLITTING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Train set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # STEP 4: Train models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Train XGBoost\n",
    "    models['xgboost'] = train_xgboost(X_train, y_train, X_test, y_test)\n",
    "    plot_feature_importance(models['xgboost'], X.columns.tolist(), 'XGBoost')\n",
    "    \n",
    "    # Train Random Forest\n",
    "    models['random_forest'] = train_random_forest(X_train, y_train, X_test, y_test)\n",
    "    plot_feature_importance(models['random_forest'], X.columns.tolist(), 'Random Forest')\n",
    "    \n",
    "    # Train SVM\n",
    "    models['svm'] = train_svm(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # STEP 5: Save models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: SAVING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        filename = f\"diabetes_{model_name}_{timestamp}.joblib\"\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'target_encoder': target_encoder,\n",
    "            'feature_names': X.columns.tolist(),\n",
    "            'target_column': TARGET_COLUMN\n",
    "        }, filename)\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved {model_name} to: {filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE! ðŸŽ‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nâœ“ Models trained and saved:\")\n",
    "    print(f\"  - diabetes_xgboost_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_random_forest_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_svm_{timestamp}.joblib\")\n",
    "    \n",
    "    print(\"\\nâœ“ Feature importance plots generated:\")\n",
    "    print(\"  - diabetes_xgboost_feature_importance.png\")\n",
    "    print(\"  - diabetes_random_forest_feature_importance.png\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Next steps:\")\n",
    "    print(\"  1. Review the classification reports above\")\n",
    "    print(\"  2. Check feature importance plots\")\n",
    "    print(\"  3. Choose the best performing model\")\n",
    "    print(\"  4. Use the saved .joblib file for predictions\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b688756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# WellNest ML Training Pipeline - DIABETES DOMAIN\n",
      "################################################################################\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Reading CSV file: C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv\n",
      "âœ“ Loaded 86,641 rows with 32 columns\n",
      "\n",
      "Final dataset: 86,641 rows\n",
      "\n",
      "Target variable distribution:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            84618\n",
      "needs_attention     1350\n",
      "urgent               673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportions:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            0.976651\n",
      "needs_attention    0.015582\n",
      "urgent             0.007768\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE PREPARATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Selected 31 feature columns\n",
      "âœ“ Target column: GLUCOSE_URGENCY_LEVEL\n",
      "\n",
      "âœ“ Categorical features: 13\n",
      "âœ“ Numerical features: 8\n",
      "\n",
      "âœ“ Handling missing values...\n",
      "\n",
      "âœ“ Encoding categorical variables...\n",
      "  - GENDER: 3 unique values\n",
      "  - SMOKING_HISTORY: 6 unique values\n",
      "  - DBT_LOADED_AT: 1 unique values\n",
      "  - DIABETES_STAGE: 4 unique values\n",
      "  - GLUCOSE_CONTROL_STATUS: 5 unique values\n",
      "  - GLUCOSE_HBA1C_CONCORDANCE: 4 unique values\n",
      "  - BMI_CATEGORY: 7 unique values\n",
      "  - SMOKING_STATUS_CLEAN: 5 unique values\n",
      "  - HYPERGLYCEMIA_URGENCY: 3 unique values\n",
      "  - HYPOGLYCEMIA_URGENCY: 1 unique values\n",
      "  - WEIGHT_MANAGEMENT_PRIORITY: 5 unique values\n",
      "  - SMOKING_CESSATION_PRIORITY: 5 unique values\n",
      "  - AGE_RISK_CATEGORY: 3 unique values\n",
      "\n",
      "âœ“ Scaling numerical features...\n",
      "\n",
      "âœ“ Encoding target variable...\n",
      "  - Target classes: ['needs_attention' 'routine' 'urgent']\n",
      "\n",
      "âœ“ Final feature matrix shape: (86641, 31)\n",
      "\n",
      "================================================================================\n",
      "STEP 3: SPLITTING DATA (70-20-10)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Train set: 60,648 samples (70.0%)\n",
      "âœ“ Validation set: 17,337 samples (20.0%)\n",
      "âœ“ Test set: 8,656 samples (10.0%)\n",
      "\n",
      "âœ“ Train target distribution:\n",
      "1    59232\n",
      "0      945\n",
      "2      471\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ Validation target distribution:\n",
      "1    16932\n",
      "0      270\n",
      "2      135\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ Test target distribution:\n",
      "1    8454\n",
      "0     135\n",
      "2      67\n",
      "Name: count, dtype: int64\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS WITH VALIDATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: XGBoost\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with cross-validation...\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "âœ“ Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50, 'reg_alpha': 0, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable\n",
      "   Validation-Test gap: 0.0000\n",
      "   âœ“ Gap is acceptable\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16932\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17337\n",
      "   macro avg       1.00      1.00      1.00     17337\n",
      "weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       135\n",
      "           1       1.00      1.00      1.00      8454\n",
      "           2       1.00      1.00      1.00        67\n",
      "\n",
      "    accuracy                           1.00      8656\n",
      "   macro avg       1.00      1.00      1.00      8656\n",
      "weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_xgboost_feature_importance.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features:\n",
      "  1. HYPERGLYCEMIA_URGENCY: 0.3461\n",
      "  2. BLOOD_GLUCOSE_LEVEL: 0.3157\n",
      "  3. GLUCOSE_CONTROL_STATUS: 0.1693\n",
      "  4. HAS_DIABETES: 0.0662\n",
      "  5. CARDIOMETABOLIC_DISEASE_COUNT: 0.0550\n",
      "  6. HBA1C_LEVEL: 0.0318\n",
      "  7. HAS_MULTIPLE_CONDITIONS: 0.0097\n",
      "  8. HAS_PREMATURE_DISEASE: 0.0037\n",
      "  9. METABOLIC_SYNDROME_SCORE: 0.0009\n",
      "  10. BMI_CATEGORY: 0.0008\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 2: Random Forest\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with cross-validation...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "âœ“ Best parameters: {'max_depth': 10, 'max_features': 'sqrt', 'max_samples': 0.8, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16932\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17337\n",
      "   macro avg       1.00      1.00      1.00     17337\n",
      "weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       135\n",
      "           1       1.00      1.00      1.00      8454\n",
      "           2       1.00      1.00      1.00        67\n",
      "\n",
      "    accuracy                           1.00      8656\n",
      "   macro avg       1.00      1.00      1.00      8656\n",
      "weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_random_forest_feature_importance.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features:\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.4074\n",
      "  2. HYPERGLYCEMIA_URGENCY: 0.3321\n",
      "  3. GLUCOSE_CONTROL_STATUS: 0.1302\n",
      "  4. HAS_DIABETES: 0.0557\n",
      "  5. CARDIOMETABOLIC_DISEASE_COUNT: 0.0221\n",
      "  6. HBA1C_LEVEL: 0.0123\n",
      "  7. GLUCOSE_HBA1C_CONCORDANCE: 0.0076\n",
      "  8. CARDIOVASCULAR_RISK_SCORE: 0.0073\n",
      "  9. DIABETES_COMPLICATION_RISK_SCORE: 0.0042\n",
      "  10. HAS_MULTIPLE_CONDITIONS: 0.0031\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 3: SVM\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with cross-validation...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "âœ“ Best parameters: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SVM Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       270\n",
      "           1       1.00      1.00      1.00     16932\n",
      "           2       1.00      1.00      1.00       135\n",
      "\n",
      "    accuracy                           1.00     17337\n",
      "   macro avg       1.00      1.00      1.00     17337\n",
      "weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       135\n",
      "           1       1.00      1.00      1.00      8454\n",
      "           2       1.00      1.00      1.00        67\n",
      "\n",
      "    accuracy                           1.00      8656\n",
      "   macro avg       1.00      1.00      1.00      8656\n",
      "weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  Class 0: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  Class 1: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  Class 2: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "================================================================================\n",
      "STEP 5: SAVING MODELS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved xgboost to: diabetes_xgboost_20251024_173251.joblib\n",
      "\n",
      "âœ“ Saved random_forest to: diabetes_random_forest_20251024_173251.joblib\n",
      "\n",
      "âœ“ Saved svm to: diabetes_svm_20251024_173251.joblib\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE! ðŸŽ‰\n",
      "================================================================================\n",
      "\n",
      "âœ“ Models trained and saved:\n",
      "  - diabetes_xgboost_20251024_173251.joblib\n",
      "  - diabetes_random_forest_20251024_173251.joblib\n",
      "  - diabetes_svm_20251024_173251.joblib\n",
      "\n",
      "âœ“ Feature importance plots generated:\n",
      "  - diabetes_xgboost_feature_importance.png\n",
      "  - diabetes_random_forest_feature_importance.png\n",
      "\n",
      "ðŸ’¡ Next steps:\n",
      "  1. Review the classification reports above\n",
      "  2. Check feature importance plots\n",
      "  3. Choose the best performing model\n",
      "  4. Use the saved .joblib file for predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Modeling Diabetes DataSet\n",
    "70-20-10\n",
    "\"\"\"\n",
    "\n",
    "# Your CSV file name\n",
    "CSV_FILE =r'C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv'  # â† Change this to your file name\n",
    "\n",
    "# Your target column name (the urgency level you want to predict)\n",
    "TARGET_COLUMN = 'GLUCOSE_URGENCY_LEVEL'  # â† Change if your column name is different\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_diabetes_data(csv_file, target_col):\n",
    "    \"\"\"Load diabetes feature data from CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nReading CSV file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"\\nâŒ ERROR: Target column '{target_col}' not found!\")\n",
    "        print(f\"Available columns: {df.columns.tolist()[:10]}...\")\n",
    "        return None\n",
    "    \n",
    "    # Remove rows with missing target values\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    \n",
    "    if removed_rows > 0:\n",
    "        print(f\"âœ“ Removed {removed_rows:,} rows with missing target values\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df):,} rows\")\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget proportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: PREPARE FEATURES\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare features for ML training:\n",
    "    - Remove non-feature columns\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables\n",
    "    - Scale numerical features\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: FEATURE PREPARATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Columns to exclude from features (add any others specific to your data)\n",
    "    exclude_cols = [\n",
    "        target_col,\n",
    "        'patient_id', 'record_id', 'id', 'index',\n",
    "        'created_at', 'updated_at', 'timestamp',\n",
    "        # Remove Tier 3 conversational flags (if they exist)\n",
    "        'should_ask_diabetic_symptoms',\n",
    "        'should_ask_cardiovascular_symptoms',\n",
    "        'should_ask_diet_habits',\n",
    "        'should_ask_physical_activity',\n",
    "        'should_ask_medication_adherence',\n",
    "        'needs_specialist_referral_flag',\n",
    "        'priority_education_topics'\n",
    "    ]\n",
    "    \n",
    "    # Get feature columns (exclude non-features)\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in exclude_cols and col in df.columns]\n",
    "    \n",
    "    print(f\"\\nâœ“ Selected {len(feature_cols)} feature columns\")\n",
    "    print(f\"âœ“ Target column: {target_col}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ“ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"âœ“ Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nâœ“ Handling missing values...\")\n",
    "    \n",
    "    # For numerical columns: fill with median\n",
    "    for col in numerical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with median\")\n",
    "    \n",
    "    # For categorical columns: fill with mode or 'unknown'\n",
    "    for col in categorical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_value = X[col].mode()[0] if not X[col].mode().empty else 'unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\nâœ“ Encoding categorical variables...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  - {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nâœ“ Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(\"\\nâœ“ Encoding target variable...\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    print(f\"  - Target classes: {target_encoder.classes_}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoders, scaler, target_encoder\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: TRAIN MODELS\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train XGBoost model with validation set to prevent overfitting\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 1: XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reduced hyperparameter grid to prevent overfitting\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5],  # Reduced from [3, 5, 7]\n",
    "        'learning_rate': [0.01, 0.1],  # Slower learning\n",
    "        'n_estimators': [50, 100, 200],  # Added early stopping\n",
    "        'min_child_weight': [3, 5],  # Increased to prevent overfitting\n",
    "        'subsample': [0.8],  # Use only 80% of data per tree\n",
    "        'colsample_bytree': [0.8],  # Use only 80% of features per tree\n",
    "        'reg_alpha': [0, 0.1],  # L1 regularization\n",
    "        'reg_lambda': [1, 10]  # L2 regularization\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with cross-validation...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, \n",
    "        param_grid, \n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"XGBoost Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Training set\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\")\n",
    "    \n",
    "    # Validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Test set\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    val_test_gap = val_acc - test_acc\n",
    "    \n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable\")\n",
    "    \n",
    "    print(f\"   Validation-Test gap: {val_test_gap:.4f}\")\n",
    "    if abs(val_test_gap) > 0.05:\n",
    "        print(f\"   âš ï¸  Large gap - validation set may not be representative!\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train Random Forest model with validation set to prevent overfitting\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 2: Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reduced parameters to prevent overfitting\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20],  # Limited depth\n",
    "        'min_samples_split': [5, 10],  # Increased minimum\n",
    "        'min_samples_leaf': [2, 4],  # Increased minimum\n",
    "        'max_features': ['sqrt'],  # Use sqrt of features\n",
    "        'max_samples': [0.8]  # Bootstrap with 80% of data\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with cross-validation...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Random Forest Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    \"\"\"Train SVM model with validation set to prevent overfitting\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 3: SVM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # SVM with regularization to prevent overfitting\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],  # Regularization parameter\n",
    "        'kernel': ['rbf'],  # RBF kernel only\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svm = SVC(random_state=42, probability=True)\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with cross-validation...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        svm, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SVM Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - possible overfitting!\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):\n",
    "        print(f\"  Class {i}: Precision={p:.4f}, Recall={r:.4f}, F1={f:.4f}, Support={s}\")\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"Top 20 Feature Importance - {model_name}\")\n",
    "        plt.bar(range(20), importances[indices])\n",
    "        plt.xticks(range(20), [feature_names[i] for i in indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"diabetes_{model_name.lower().replace(' ', '_')}_feature_importance.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved feature importance plot: {filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Top 10 Most Important Features:\")\n",
    "        for i in range(min(10, len(indices))):\n",
    "            print(f\"  {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# WellNest ML Training Pipeline - DIABETES DOMAIN\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # STEP 1: Load data\n",
    "    df = load_diabetes_data(CSV_FILE, TARGET_COLUMN)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nâŒ Failed to load data. Please check your CSV file and target column name.\")\n",
    "        return\n",
    "    \n",
    "    # STEP 2: Prepare features\n",
    "    X, y, label_encoders, scaler, target_encoder = prepare_features(df, TARGET_COLUMN)\n",
    "    \n",
    "    # STEP 3: Split data (70% train, 20% validation, 10% test)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: SPLITTING DATA (70-20-10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # First split: 70% train, 30% temp (which will become validation + test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3,  # 30% for validation + test\n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: Split the 30% into 20% validation and 10% test\n",
    "    # 20/(20+10) = 0.6667 of the temp set becomes validation\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.333,  # 1/3 of 30% = 10% of total\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Train set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nâœ“ Train target distribution:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "    print(\"\\nâœ“ Validation target distribution:\")\n",
    "    print(pd.Series(y_val).value_counts())\n",
    "    print(\"\\nâœ“ Test target distribution:\")\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "    \n",
    "    # STEP 4: Train models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MODELS WITH VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Train XGBoost\n",
    "    models['xgboost'] = train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    plot_feature_importance(models['xgboost'], X.columns.tolist(), 'XGBoost')\n",
    "    \n",
    "    # Train Random Forest\n",
    "    models['random_forest'] = train_random_forest(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    plot_feature_importance(models['random_forest'], X.columns.tolist(), 'Random Forest')\n",
    "    \n",
    "    # Train SVM\n",
    "    models['svm'] = train_svm(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \n",
    "    # STEP 5: Save models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: SAVING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        filename = f\"diabetes_{model_name}_{timestamp}.joblib\"\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'target_encoder': target_encoder,\n",
    "            'feature_names': X.columns.tolist(),\n",
    "            'target_column': TARGET_COLUMN\n",
    "        }, filename)\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved {model_name} to: {filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE! ðŸŽ‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nâœ“ Models trained and saved:\")\n",
    "    print(f\"  - diabetes_xgboost_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_random_forest_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_svm_{timestamp}.joblib\")\n",
    "    \n",
    "    print(\"\\nâœ“ Feature importance plots generated:\")\n",
    "    print(\"  - diabetes_xgboost_feature_importance.png\")\n",
    "    print(\"  - diabetes_random_forest_feature_importance.png\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Next steps:\")\n",
    "    print(\"  1. Review the classification reports above\")\n",
    "    print(\"  2. Check feature importance plots\")\n",
    "    print(\"  3. Choose the best performing model\")\n",
    "    print(\"  4. Use the saved .joblib file for predictions\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "141da4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# WellNest ML Training Pipeline - DIABETES DOMAIN (FIXED)\n",
      "# - Data leakage prevention\n",
      "# - Class imbalance handling with CLASS WEIGHTS\n",
      "# - Proper validation\n",
      "################################################################################\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Reading CSV file: C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv\n",
      "âœ“ Loaded 86,641 rows with 32 columns\n",
      "\n",
      "Final dataset: 86,641 rows\n",
      "\n",
      "Target variable distribution:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            84618\n",
      "needs_attention     1350\n",
      "urgent               673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportions:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            0.976651\n",
      "needs_attention    0.015582\n",
      "urgent             0.007768\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "âš ï¸  WARNING: Severe class imbalance detected!\n",
      "   Smallest class: 0.78%\n",
      "   Will use SMOTE to balance classes during training\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE PREPARATION (LEAKAGE PREVENTION)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Excluded 8 columns to prevent data leakage\n",
      "âœ“ Selected 24 feature columns for training\n",
      "âœ“ Target column: GLUCOSE_URGENCY_LEVEL\n",
      "\n",
      "ðŸ›¡ï¸ Leakage prevention - excluded features:\n",
      "   âœ“ Removed: HYPERGLYCEMIA_URGENCY\n",
      "   âœ“ Removed: HYPOGLYCEMIA_URGENCY\n",
      "   âœ“ Removed: GLUCOSE_CONTROL_STATUS\n",
      "   âœ“ Removed: GLUCOSE_HBA1C_CONCORDANCE\n",
      "\n",
      "Remaining feature columns:\n",
      "  ['GENDER', 'AGE', 'HAS_HYPERTENSION', 'HAS_HEART_DISEASE', 'SMOKING_HISTORY', 'BMI', 'HBA1C_LEVEL', 'BLOOD_GLUCOSE_LEVEL', 'HAS_DIABETES', 'DIABETES_STAGE', 'BMI_CATEGORY', 'IS_OBESE', 'IS_SEVERELY_OBESE', 'CARDIOMETABOLIC_DISEASE_COUNT', 'HAS_TRIPLE_DIAGNOSIS', 'HAS_MULTIPLE_CONDITIONS', 'SMOKING_STATUS_CLEAN', 'IS_CURRENT_SMOKER', 'HAS_SMOKING_HISTORY', 'CARDIOVASCULAR_RISK_SCORE', 'METABOLIC_SYNDROME_SCORE', 'DIABETES_COMPLICATION_RISK_SCORE', 'AGE_RISK_CATEGORY', 'HAS_PREMATURE_DISEASE']\n",
      "\n",
      "âœ“ Categorical features: 6\n",
      "âœ“ Numerical features: 8\n",
      "\n",
      "âœ“ Handling missing values...\n",
      "\n",
      "âœ“ Encoding categorical variables...\n",
      "  - GENDER: 3 unique values\n",
      "  - SMOKING_HISTORY: 6 unique values\n",
      "  - DIABETES_STAGE: 4 unique values\n",
      "  - BMI_CATEGORY: 7 unique values\n",
      "  - SMOKING_STATUS_CLEAN: 5 unique values\n",
      "  - AGE_RISK_CATEGORY: 3 unique values\n",
      "\n",
      "âœ“ Scaling numerical features...\n",
      "\n",
      "âœ“ Encoding target variable...\n",
      "  - Target classes: ['needs_attention' 'routine' 'urgent']\n",
      "\n",
      "âœ“ Final feature matrix shape: (86641, 24)\n",
      "\n",
      "================================================================================\n",
      "STEP 3A: SPLITTING DATA (70-20-10)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Train set: 60,648 samples (70.0%)\n",
      "âœ“ Validation set: 17,337 samples (20.0%)\n",
      "âœ“ Test set: 8,656 samples (10.0%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\n",
      "================================================================================\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 945 samples (1.56%)\n",
      "  Class 1: 59,232 samples (97.67%)\n",
      "  Class 2: 471 samples (0.78%)\n",
      "\n",
      "âœ“ Computed class weights:\n",
      "  Class 0: weight = 21.3926\n",
      "  Class 1: weight = 0.3413\n",
      "  Class 2: weight = 42.9214\n",
      "\n",
      "ðŸ’¡ These weights will give more importance to minority classes during training\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS (WITH CLASS WEIGHTS)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: XGBoost\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with 5-fold cross-validation...\n",
      "   (Using class weights to handle imbalance)\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "\n",
      "âœ“ Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable - good generalization\n",
      "   Validation-Test gap: 0.0000\n",
      "   âœ“ Consistent performance across validation and test\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       270\n",
      "        routine       1.00      1.00      1.00     16932\n",
      "         urgent       1.00      1.00      1.00       135\n",
      "\n",
      "       accuracy                           1.00     17337\n",
      "      macro avg       1.00      1.00      1.00     17337\n",
      "   weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results (Final Performance):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      1.00      1.00        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_xgboost_feature_importance_fixed.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features (No Leakage):\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.5839\n",
      "  2. CARDIOVASCULAR_RISK_SCORE: 0.1342\n",
      "  3. CARDIOMETABOLIC_DISEASE_COUNT: 0.1229\n",
      "  4. HAS_DIABETES: 0.1130\n",
      "  5. HBA1C_LEVEL: 0.0144\n",
      "  6. METABOLIC_SYNDROME_SCORE: 0.0057\n",
      "  7. SMOKING_HISTORY: 0.0034\n",
      "  8. BMI_CATEGORY: 0.0034\n",
      "  9. DIABETES_COMPLICATION_RISK_SCORE: 0.0033\n",
      "  10. DIABETES_STAGE: 0.0026\n",
      "\n",
      "   âœ“ No leaky features in top 10 - good!\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 2: Random Forest\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with 5-fold cross-validation...\n",
      "   (Using class weights to handle imbalance)\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "\n",
      "âœ“ Best parameters: {'max_depth': 10, 'max_features': 'sqrt', 'max_samples': 0.8, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable - good generalization\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       270\n",
      "        routine       1.00      1.00      1.00     16932\n",
      "         urgent       1.00      1.00      1.00       135\n",
      "\n",
      "       accuracy                           1.00     17337\n",
      "      macro avg       1.00      1.00      1.00     17337\n",
      "   weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results (Final Performance):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      1.00      1.00        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "âœ“ Saved feature importance plot: diabetes_random_forest_feature_importance_fixed.png\n",
      "\n",
      "âœ“ Top 10 Most Important Features (No Leakage):\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.5363\n",
      "  2. HAS_DIABETES: 0.1509\n",
      "  3. CARDIOMETABOLIC_DISEASE_COUNT: 0.1020\n",
      "  4. CARDIOVASCULAR_RISK_SCORE: 0.0676\n",
      "  5. HBA1C_LEVEL: 0.0289\n",
      "  6. METABOLIC_SYNDROME_SCORE: 0.0238\n",
      "  7. DIABETES_COMPLICATION_RISK_SCORE: 0.0172\n",
      "  8. AGE: 0.0163\n",
      "  9. BMI: 0.0135\n",
      "  10. DIABETES_STAGE: 0.0100\n",
      "\n",
      "   âœ“ No leaky features in top 10 - good!\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 3: SVM\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning with 5-fold cross-validation...\n",
      "   (Using class weights to handle imbalance)\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "âœ“ Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "âœ“ Best CV score (train): 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SVM Results on ALL Sets:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Set Accuracy: 1.0000\n",
      "   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\n",
      "ðŸ“Š VALIDATION Set Accuracy: 1.0000\n",
      "ðŸ“Š TEST Set Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Overfitting Analysis:\n",
      "   Train-Validation gap: 0.0000\n",
      "   âœ“ Gap is acceptable - good generalization\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed VALIDATION Set Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       270\n",
      "        routine       1.00      1.00      1.00     16932\n",
      "         urgent       1.00      1.00      1.00       135\n",
      "\n",
      "       accuracy                           1.00     17337\n",
      "      macro avg       1.00      1.00      1.00     17337\n",
      "   weighted avg       1.00      1.00      1.00     17337\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  270     0     0]\n",
      " [    0 16932     0]\n",
      " [    0     0   135]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=270\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=16932\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Detailed TEST Set Results (Final Performance):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      1.00      1.00        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "Per-Class Metrics:\n",
      "  needs_attention: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=135\n",
      "  routine: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=8454\n",
      "  urgent: Precision=1.0000, Recall=1.0000, F1=1.0000, Support=67\n",
      "\n",
      "================================================================================\n",
      "STEP 5: SAVING MODELS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved xgboost to: diabetes_xgboost_fixed_20251024_181619.joblib\n",
      "\n",
      "âœ“ Saved random_forest to: diabetes_random_forest_fixed_20251024_181619.joblib\n",
      "\n",
      "âœ“ Saved svm to: diabetes_svm_fixed_20251024_181619.joblib\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE! ðŸŽ‰\n",
      "================================================================================\n",
      "\n",
      "âœ“ Models trained and saved (leakage-free):\n",
      "  - diabetes_xgboost_fixed_20251024_181619.joblib\n",
      "  - diabetes_random_forest_fixed_20251024_181619.joblib\n",
      "  - diabetes_svm_fixed_20251024_181619.joblib\n",
      "\n",
      "âœ“ Feature importance plots generated:\n",
      "  - diabetes_xgboost_feature_importance_fixed.png\n",
      "  - diabetes_random_forest_feature_importance_fixed.png\n",
      "\n",
      "ðŸ’¡ Expected realistic performance:\n",
      "  âœ“ Train accuracy: 75-90%\n",
      "  âœ“ Validation accuracy: 70-85%\n",
      "  âœ“ Test accuracy: 70-85%\n",
      "  âœ“ Train-Val gap: < 10%\n",
      "\n",
      "âš ï¸  If you still see 95%+ accuracy, there may be other leaky features.\n",
      "   Check the feature importance plots to identify them.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WellNest Healthcare ML Model Training - DIABETES ONLY (FIXED VERSION)\n",
    "Trains XGBoost, Random Forest, and SVM models for diabetes triage prediction\n",
    "\n",
    "FIXES:\n",
    "- Removed data leakage features\n",
    "- Added SMOTE for class imbalance\n",
    "- Added proper feature importance analysis\n",
    "- Enhanced overfitting detection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - UPDATE THIS\n",
    "# =============================================================================\n",
    "\n",
    "# Your CSV file name\n",
    "CSV_FILE = r'C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv'\n",
    "\n",
    "# Your target column name (the urgency level you want to predict)\n",
    "TARGET_COLUMN = 'GLUCOSE_URGENCY_LEVEL'\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_diabetes_data(csv_file, target_col):\n",
    "    \"\"\"Load diabetes feature data from CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nReading CSV file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"\\nâŒ ERROR: Target column '{target_col}' not found!\")\n",
    "        print(f\"Available columns: {df.columns.tolist()[:10]}...\")\n",
    "        return None\n",
    "    \n",
    "    # Remove rows with missing target values\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    \n",
    "    if removed_rows > 0:\n",
    "        print(f\"âœ“ Removed {removed_rows:,} rows with missing target values\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df):,} rows\")\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget proportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    # Warn about class imbalance\n",
    "    class_proportions = df[target_col].value_counts(normalize=True)\n",
    "    min_class_prop = class_proportions.min()\n",
    "    if min_class_prop < 0.1:\n",
    "        print(f\"\\nâš ï¸  WARNING: Severe class imbalance detected!\")\n",
    "        print(f\"   Smallest class: {min_class_prop*100:.2f}%\")\n",
    "        print(f\"   Will use SMOTE to balance classes during training\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: PREPARE FEATURES (WITH LEAKAGE PREVENTION)\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare features for ML training:\n",
    "    - Remove data leakage features\n",
    "    - Remove non-feature columns\n",
    "    - Handle missing values\n",
    "    - Encode categorical variables\n",
    "    - Scale numerical features\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: FEATURE PREPARATION (LEAKAGE PREVENTION)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ðŸš¨ CRITICAL: Columns to exclude to prevent data leakage\n",
    "    exclude_cols = [\n",
    "        target_col,\n",
    "        \n",
    "        # ID and timestamp columns\n",
    "        'patient_id', 'record_id', 'id', 'index',\n",
    "        'created_at', 'updated_at', 'timestamp', 'DBT_LOADED_AT',\n",
    "        \n",
    "        # ðŸš¨ DATA LEAKAGE FEATURES - These directly calculate the target!\n",
    "        'HYPERGLYCEMIA_URGENCY',  # Directly used to calculate GLUCOSE_URGENCY_LEVEL\n",
    "        'HYPOGLYCEMIA_URGENCY',   # Directly used to calculate GLUCOSE_URGENCY_LEVEL\n",
    "        'GLUCOSE_CONTROL_STATUS', # Derived from same logic as target\n",
    "        'GLUCOSE_HBA1C_CONCORDANCE', # Derived feature that leaks information\n",
    "        \n",
    "        # Tier 3 conversational flags (not useful for prediction)\n",
    "        'should_ask_diabetic_symptoms',\n",
    "        'should_ask_cardiovascular_symptoms',\n",
    "        'should_ask_diet_habits',\n",
    "        'should_ask_physical_activity',\n",
    "        'should_ask_medication_adherence',\n",
    "        'should_ask_mental_health_screening',\n",
    "        'should_ask_sleep_quality',\n",
    "        'should_ask_substance_use',\n",
    "        'should_ask_prenatal_history',\n",
    "        'should_ask_menstrual_history',\n",
    "        'needs_specialist_referral_flag',\n",
    "        'priority_education_topics',\n",
    "        \n",
    "        # Other potentially leaky derived features\n",
    "        'WEIGHT_MANAGEMENT_PRIORITY',  # Might be derived from target\n",
    "        'SMOKING_CESSATION_PRIORITY'   # Might be derived from target\n",
    "    ]\n",
    "    \n",
    "    # Get feature columns (exclude non-features and leaky features)\n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in exclude_cols and col in df.columns]\n",
    "    \n",
    "    print(f\"\\nâœ“ Excluded {len([c for c in exclude_cols if c in df.columns])} columns to prevent data leakage\")\n",
    "    print(f\"âœ“ Selected {len(feature_cols)} feature columns for training\")\n",
    "    print(f\"âœ“ Target column: {target_col}\")\n",
    "    \n",
    "    print(\"\\nðŸ›¡ï¸ Leakage prevention - excluded features:\")\n",
    "    leaky_features = ['HYPERGLYCEMIA_URGENCY', 'HYPOGLYCEMIA_URGENCY', \n",
    "                     'GLUCOSE_CONTROL_STATUS', 'GLUCOSE_HBA1C_CONCORDANCE']\n",
    "    for feat in leaky_features:\n",
    "        if feat in df.columns:\n",
    "            print(f\"   âœ“ Removed: {feat}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    print(f\"\\nRemaining feature columns:\")\n",
    "    print(f\"  {feature_cols}\")\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ“ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"âœ“ Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nâœ“ Handling missing values...\")\n",
    "    \n",
    "    # For numerical columns: fill with median\n",
    "    for col in numerical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with median\")\n",
    "    \n",
    "    # For categorical columns: fill with mode or 'unknown'\n",
    "    for col in categorical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_value = X[col].mode()[0] if not X[col].mode().empty else 'unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\nâœ“ Encoding categorical variables...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  - {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nâœ“ Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(\"\\nâœ“ Encoding target variable...\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    print(f\"  - Target classes: {target_encoder.classes_}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoders, scaler, target_encoder, feature_cols\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: HANDLE CLASS IMBALANCE WITH CLASS WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_sample_weights(y_train):\n",
    "    \"\"\"Compute class weights to handle imbalance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTraining set class distribution:\")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count:,} samples ({count/len(y_train)*100:.2f}%)\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weights = np.zeros(len(y_train))\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        sample_weights[y_train == cls] = weight\n",
    "    \n",
    "    print(f\"\\nâœ“ Computed class weights:\")\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        print(f\"  Class {cls}: weight = {weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ These weights will give more importance to minority classes during training\")\n",
    "    \n",
    "    return sample_weights, dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: TRAIN MODELS\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, sample_weights):\n",
    "    \"\"\"Train XGBoost model with validation set and class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 1: XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reduced hyperparameter grid to prevent overfitting\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'min_child_weight': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0.1, 1],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with 5-fold cross-validation...\")\n",
    "    print(\"   (Using class weights to handle imbalance)\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit with sample weights\n",
    "    grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"XGBoost Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Training set\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\")\n",
    "    elif train_acc > 0.85:\n",
    "        print(\"   âœ“ Good training accuracy\")\n",
    "    \n",
    "    # Validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Test set\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    val_test_gap = val_acc - test_acc\n",
    "    \n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.10:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    elif train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Moderate gap - some overfitting present\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable - good generalization\")\n",
    "    \n",
    "    print(f\"   Validation-Test gap: {val_test_gap:.4f}\")\n",
    "    if abs(val_test_gap) > 0.05:\n",
    "        print(f\"   âš ï¸  Large gap - validation set may not be representative\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Consistent performance across validation and test\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred, target_encoder)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results (Final Performance):\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train Random Forest model with validation set and class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 2: Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Reduced parameters to prevent overfitting\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_samples': [0.8]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42, \n",
    "        n_jobs=-1,\n",
    "        class_weight=class_weights_dict  # Use class weights\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with 5-fold cross-validation...\")\n",
    "    print(\"   (Using class weights to handle imbalance)\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Random Forest Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\")\n",
    "    elif train_acc > 0.85:\n",
    "        print(\"   âœ“ Good training accuracy\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.10:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    elif train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Moderate gap - some overfitting present\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable - good generalization\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred, target_encoder)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results (Final Performance):\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train SVM model with validation set and class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 3: SVM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # SVM with regularization to prevent overfitting\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svm = SVC(\n",
    "        random_state=42, \n",
    "        probability=True,\n",
    "        class_weight=class_weights_dict  # Use class weights\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning with 5-fold cross-validation...\")\n",
    "    print(\"   (Using class weights to handle imbalance)\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        svm, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score (train): {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all three sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SVM Results on ALL Sets:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Set Accuracy: {train_acc:.4f}\")\n",
    "    if train_acc > 0.95:\n",
    "        print(\"   âš ï¸  WARNING: Very high training accuracy - check for remaining leakage!\")\n",
    "    elif train_acc > 0.85:\n",
    "        print(\"   âœ“ Good training accuracy\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Set Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Set Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Overfitting Analysis:\")\n",
    "    print(f\"   Train-Validation gap: {train_val_gap:.4f}\")\n",
    "    if train_val_gap > 0.10:\n",
    "        print(f\"   âš ï¸  Large gap detected - model is overfitting!\")\n",
    "    elif train_val_gap > 0.05:\n",
    "        print(f\"   âš ï¸  Moderate gap - some overfitting present\")\n",
    "    else:\n",
    "        print(f\"   âœ“ Gap is acceptable - good generalization\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed VALIDATION Set Results:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_val, y_val_pred, target_encoder)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Detailed TEST Set Results (Final Performance):\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(y_test, y_pred, target_encoder):\n",
    "    \"\"\"Comprehensive model evaluation with class names\"\"\"\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    # Get class names for better readability\n",
    "    target_names = target_encoder.classes_\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate per-class metrics with names\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    for i, (p, r, f, s) in enumerate(zip(precision, recall, f1, support)):\n",
    "        class_name = target_names[i] if i < len(target_names) else f\"Class {i}\"\n",
    "        print(f\"  {class_name}: Precision={p:.4f}, Recall={r:.4f}, F1={f:.4f}, Support={s}\")\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:20]  # Top 20 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"Top 20 Feature Importance - {model_name} (No Leakage)\")\n",
    "        plt.bar(range(20), importances[indices])\n",
    "        plt.xticks(range(20), [feature_names[i] for i in indices], rotation=90)\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"diabetes_{model_name.lower().replace(' ', '_')}_feature_importance_fixed.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved feature importance plot: {filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Top 10 Most Important Features (No Leakage):\")\n",
    "        for i in range(min(10, len(indices))):\n",
    "            feat_name = feature_names[indices[i]]\n",
    "            feat_importance = importances[indices[i]]\n",
    "            print(f\"  {i+1}. {feat_name}: {feat_importance:.4f}\")\n",
    "            \n",
    "        # Check if leaky features appear in top 10\n",
    "        leaky_features = ['HYPERGLYCEMIA_URGENCY', 'HYPOGLYCEMIA_URGENCY', \n",
    "                         'GLUCOSE_CONTROL_STATUS', 'GLUCOSE_HBA1C_CONCORDANCE']\n",
    "        top_10_features = [feature_names[indices[i]] for i in range(min(10, len(indices)))]\n",
    "        \n",
    "        found_leaky = [f for f in leaky_features if f in top_10_features]\n",
    "        if found_leaky:\n",
    "            print(f\"\\n   âš ï¸  WARNING: Leaky features still in top 10: {found_leaky}\")\n",
    "            print(f\"       These should have been excluded!\")\n",
    "        else:\n",
    "            print(f\"\\n   âœ“ No leaky features in top 10 - good!\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline with leakage prevention\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# WellNest ML Training Pipeline - DIABETES DOMAIN (FIXED)\")\n",
    "    print(\"# - Data leakage prevention\")\n",
    "    print(\"# - Class imbalance handling with CLASS WEIGHTS\")\n",
    "    print(\"# - Proper validation\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # STEP 1: Load data\n",
    "    df = load_diabetes_data(CSV_FILE, TARGET_COLUMN)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"\\nâŒ Failed to load data. Please check your CSV file and target column name.\")\n",
    "        return\n",
    "    \n",
    "    # STEP 2: Prepare features (with leakage prevention)\n",
    "    X, y, label_encoders, scaler, target_encoder, feature_names = prepare_features(df, TARGET_COLUMN)\n",
    "    \n",
    "    # STEP 3A: Split data (70% train, 20% validation, 10% test)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3A: SPLITTING DATA (70-20-10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # First split: 70% train, 30% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3,\n",
    "        random_state=42, \n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Second split: 20% validation, 10% test\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.333,  # 1/3 of 30% = 10% of total\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Train set: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Validation set: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Test set: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # STEP 3B: Compute class weights for imbalance\n",
    "    sample_weights, class_weights_dict = compute_sample_weights(y_train)\n",
    "    \n",
    "    # STEP 4: Train models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MODELS (WITH CLASS WEIGHTS)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # Train XGBoost\n",
    "    models['xgboost'] = train_xgboost(\n",
    "        X_train, y_train, \n",
    "        X_val, y_val, \n",
    "        X_test, y_test,\n",
    "        target_encoder,\n",
    "        sample_weights\n",
    "    )\n",
    "    plot_feature_importance(models['xgboost'], feature_names, 'XGBoost')\n",
    "    \n",
    "    # Train Random Forest\n",
    "    models['random_forest'] = train_random_forest(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        target_encoder,\n",
    "        class_weights_dict\n",
    "    )\n",
    "    plot_feature_importance(models['random_forest'], feature_names, 'Random Forest')\n",
    "    \n",
    "    # Train SVM\n",
    "    models['svm'] = train_svm(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test,\n",
    "        target_encoder,\n",
    "        class_weights_dict\n",
    "    )\n",
    "    \n",
    "    # STEP 5: Save models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: SAVING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        filename = f\"diabetes_{model_name}_fixed_{timestamp}.joblib\"\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'target_encoder': target_encoder,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': TARGET_COLUMN\n",
    "        }, filename)\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved {model_name} to: {filename}\")\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE! ðŸŽ‰\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nâœ“ Models trained and saved (leakage-free):\")\n",
    "    print(f\"  - diabetes_xgboost_fixed_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_random_forest_fixed_{timestamp}.joblib\")\n",
    "    print(f\"  - diabetes_svm_fixed_{timestamp}.joblib\")\n",
    "    \n",
    "    print(\"\\nâœ“ Feature importance plots generated:\")\n",
    "    print(\"  - diabetes_xgboost_feature_importance_fixed.png\")\n",
    "    print(\"  - diabetes_random_forest_feature_importance_fixed.png\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Expected realistic performance:\")\n",
    "    print(\"  âœ“ Train accuracy: 75-90%\")\n",
    "    print(\"  âœ“ Validation accuracy: 70-85%\")\n",
    "    print(\"  âœ“ Test accuracy: 70-85%\")\n",
    "    print(\"  âœ“ Train-Val gap: < 10%\")\n",
    "    \n",
    "    print(\"\\nâš ï¸  If you still see 95%+ accuracy, there may be other leaky features.\")\n",
    "    print(\"   Check the feature importance plots to identify them.\")\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f059c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# WellNest ML Training - DIABETES (ULTRA-STRICT VERSION)\n",
      "# Using ONLY raw clinical measurements\n",
      "################################################################################\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Reading CSV file: C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv\n",
      "âœ“ Loaded 86,641 rows with 32 columns\n",
      "\n",
      "Final dataset: 86,641 rows\n",
      "\n",
      "Target variable distribution:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            84618\n",
      "needs_attention     1350\n",
      "urgent               673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportions:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            0.976651\n",
      "needs_attention    0.015582\n",
      "urgent             0.007768\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "âš ï¸  WARNING: Severe class imbalance detected!\n",
      "   Smallest class: 0.78%\n",
      "   Will use class weights to handle imbalance\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE PREPARATION (RAW MEASUREMENTS ONLY)\n",
      "================================================================================\n",
      "\n",
      "ðŸ›¡ï¸ ULTRA-STRICT LEAKAGE PREVENTION:\n",
      "   Using ONLY raw clinical measurements that a doctor would collect\n",
      "   Excluding ALL derived/calculated features\n",
      "\n",
      "   Allowed features (10):\n",
      "     âœ“ AGE\n",
      "     âœ“ GENDER\n",
      "     âœ“ BMI\n",
      "     âœ“ HBA1C_LEVEL\n",
      "     âœ“ BLOOD_GLUCOSE_LEVEL\n",
      "     âœ“ HAS_HYPERTENSION\n",
      "     âœ“ HAS_HEART_DISEASE\n",
      "     âœ“ SMOKING_HISTORY\n",
      "     âœ“ IS_CURRENT_SMOKER\n",
      "     âœ“ HAS_SMOKING_HISTORY\n",
      "\n",
      "âœ“ Using 10 raw features for training\n",
      "âœ“ Target column: GLUCOSE_URGENCY_LEVEL\n",
      "\n",
      "âœ“ Categorical features: 2\n",
      "âœ“ Numerical features: 4\n",
      "\n",
      "âœ“ Handling missing values...\n",
      "\n",
      "âœ“ Encoding categorical variables...\n",
      "  - GENDER: 3 unique values\n",
      "  - SMOKING_HISTORY: 6 unique values\n",
      "\n",
      "âœ“ Scaling numerical features...\n",
      "\n",
      "âœ“ Encoding target variable...\n",
      "  - Target classes: ['needs_attention' 'routine' 'urgent']\n",
      "\n",
      "âœ“ Final feature matrix shape: (86641, 10)\n",
      "\n",
      "================================================================================\n",
      "STEP 3A: SPLITTING DATA (70-20-10)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Train: 60,648 (70.0%)\n",
      "âœ“ Validation: 17,337 (20.0%)\n",
      "âœ“ Test: 8,656 (10.0%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\n",
      "================================================================================\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 945 samples (1.56%)\n",
      "  Class 1: 59,232 samples (97.67%)\n",
      "  Class 2: 471 samples (0.78%)\n",
      "\n",
      "âœ“ Computed class weights:\n",
      "  Class 0: weight = 21.3926\n",
      "  Class 1: weight = 0.3413\n",
      "  Class 2: weight = 42.9214\n",
      "\n",
      "ðŸ’¡ Higher weights give more importance to minority classes\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: XGBoost\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (with class weights)...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "\n",
      "âœ“ Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.8}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "XGBoost Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Accuracy: 1.0000\n",
      "ðŸ“Š VALIDATION Accuracy: 1.0000\n",
      "ðŸ“Š TEST Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Train-Val gap: 0.0000\n",
      "   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST Set Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      1.00      1.00        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "âœ“ Saved: diabetes_xgboost_importance.png\n",
      "\n",
      "âœ“ Feature Importance Ranking:\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.8859\n",
      "  2. HBA1C_LEVEL: 0.0435\n",
      "  3. AGE: 0.0394\n",
      "  4. BMI: 0.0112\n",
      "  5. HAS_HYPERTENSION: 0.0089\n",
      "  6. GENDER: 0.0053\n",
      "  7. SMOKING_HISTORY: 0.0033\n",
      "  8. HAS_HEART_DISEASE: 0.0026\n",
      "  9. HAS_SMOKING_HISTORY: 0.0000\n",
      "  10. IS_CURRENT_SMOKER: 0.0000\n",
      "\n",
      "   âš ï¸  'BLOOD_GLUCOSE_LEVEL' dominates with 88.6%!\n",
      "       The target may be calculated directly from this feature!\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 2: Random Forest\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (with class weights)...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "âœ“ Best parameters: {'max_depth': 10, 'max_features': 'sqrt', 'max_samples': 0.8, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Random Forest Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Accuracy: 1.0000\n",
      "ðŸ“Š VALIDATION Accuracy: 1.0000\n",
      "ðŸ“Š TEST Accuracy: 1.0000\n",
      "\n",
      "ðŸ” Train-Val gap: 0.0000\n",
      "   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST Set Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       1.00      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      1.00      1.00        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   0    0   67]]\n",
      "\n",
      "âœ“ Saved: diabetes_randomforest_importance.png\n",
      "\n",
      "âœ“ Feature Importance Ranking:\n",
      "  1. BLOOD_GLUCOSE_LEVEL: 0.7989\n",
      "  2. HBA1C_LEVEL: 0.1057\n",
      "  3. AGE: 0.0527\n",
      "  4. BMI: 0.0251\n",
      "  5. HAS_HYPERTENSION: 0.0049\n",
      "  6. SMOKING_HISTORY: 0.0039\n",
      "  7. HAS_HEART_DISEASE: 0.0039\n",
      "  8. GENDER: 0.0026\n",
      "  9. HAS_SMOKING_HISTORY: 0.0015\n",
      "  10. IS_CURRENT_SMOKER: 0.0009\n",
      "\n",
      "   âš ï¸  'BLOOD_GLUCOSE_LEVEL' dominates with 79.9%!\n",
      "       The target may be calculated directly from this feature!\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 3: SVM\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (with class weights)...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "\n",
      "âœ“ Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "âœ“ Best CV score: 1.0000\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SVM Results:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“Š TRAIN Accuracy: 1.0000\n",
      "ðŸ“Š VALIDATION Accuracy: 1.0000\n",
      "ðŸ“Š TEST Accuracy: 0.9999\n",
      "\n",
      "ðŸ” Train-Val gap: 0.0000\n",
      "   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TEST Set Classification Report:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Accuracy: 0.9999\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "needs_attention       0.99      1.00      1.00       135\n",
      "        routine       1.00      1.00      1.00      8454\n",
      "         urgent       1.00      0.99      0.99        67\n",
      "\n",
      "       accuracy                           1.00      8656\n",
      "      macro avg       1.00      1.00      1.00      8656\n",
      "   weighted avg       1.00      1.00      1.00      8656\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 135    0    0]\n",
      " [   0 8454    0]\n",
      " [   1    0   66]]\n",
      "\n",
      "================================================================================\n",
      "STEP 5: SAVING MODELS\n",
      "================================================================================\n",
      "âœ“ Saved: diabetes_xgboost_raw_20251024_184929.joblib\n",
      "âœ“ Saved: diabetes_random_forest_raw_20251024_184929.joblib\n",
      "âœ“ Saved: diabetes_svm_raw_20251024_184929.joblib\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ If you still see >95% accuracy:\n",
      "   â†’ Your target variable is calculated too simply from raw features\n",
      "   â†’ Example: IF glucose > 400 THEN urgent (perfect rule)\n",
      "\n",
      "ðŸ’¡ If you see 70-85% accuracy:\n",
      "   â†’ âœ… GOOD! This is realistic and academically sound\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WellNest Healthcare ML Model Training - DIABETES ONLY (FIXED VERSION)\n",
    "Trains XGBoost, Random Forest, and SVM models for diabetes triage prediction\n",
    "\n",
    "FIXES:\n",
    "- Removed ALL data leakage features\n",
    "- Uses ONLY raw clinical measurements\n",
    "- Added class weights for imbalance\n",
    "- Proper validation and overfitting detection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - UPDATE THIS\n",
    "# =============================================================================\n",
    "\n",
    "# Your CSV file name\n",
    "CSV_FILE = r'C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv'\n",
    "\n",
    "# Your target column name\n",
    "TARGET_COLUMN = 'GLUCOSE_URGENCY_LEVEL'\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_diabetes_data(csv_file, target_col):\n",
    "    \"\"\"Load diabetes feature data from CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nReading CSV file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"\\nâŒ ERROR: Target column '{target_col}' not found!\")\n",
    "        print(f\"Available columns: {df.columns.tolist()[:10]}...\")\n",
    "        return None\n",
    "    \n",
    "    # Remove rows with missing target values\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    \n",
    "    if removed_rows > 0:\n",
    "        print(f\"âœ“ Removed {removed_rows:,} rows with missing target values\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df):,} rows\")\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget proportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    # Warn about class imbalance\n",
    "    class_proportions = df[target_col].value_counts(normalize=True)\n",
    "    min_class_prop = class_proportions.min()\n",
    "    if min_class_prop < 0.1:\n",
    "        print(f\"\\nâš ï¸  WARNING: Severe class imbalance detected!\")\n",
    "        print(f\"   Smallest class: {min_class_prop*100:.2f}%\")\n",
    "        print(f\"   Will use class weights to handle imbalance\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: PREPARE FEATURES (ULTRA-STRICT - RAW ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare features using ONLY raw clinical measurements\n",
    "    This is the most conservative approach to prevent data leakage\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: FEATURE PREPARATION (RAW MEASUREMENTS ONLY)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ðŸš¨ ULTRA-STRICT: Use ONLY raw clinical measurements\n",
    "    allowed_raw_features = [\n",
    "        'AGE',\n",
    "        'GENDER',\n",
    "        'BMI',\n",
    "        'HBA1C_LEVEL',\n",
    "        'BLOOD_GLUCOSE_LEVEL',\n",
    "        'HAS_HYPERTENSION',\n",
    "        'HAS_HEART_DISEASE',\n",
    "        'SMOKING_HISTORY',\n",
    "        'IS_CURRENT_SMOKER',\n",
    "        'HAS_SMOKING_HISTORY'\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nðŸ›¡ï¸ ULTRA-STRICT LEAKAGE PREVENTION:\")\n",
    "    print(f\"   Using ONLY raw clinical measurements that a doctor would collect\")\n",
    "    print(f\"   Excluding ALL derived/calculated features\")\n",
    "    \n",
    "    print(f\"\\n   Allowed features ({len(allowed_raw_features)}):\")\n",
    "    for feat in allowed_raw_features:\n",
    "        if feat in df.columns:\n",
    "            print(f\"     âœ“ {feat}\")\n",
    "        else:\n",
    "            print(f\"     âœ— {feat} (not found in data)\")\n",
    "    \n",
    "    # Get feature columns (only raw measurements that exist)\n",
    "    feature_cols = [col for col in allowed_raw_features if col in df.columns]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"\\nâŒ ERROR: No valid features found!\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    print(f\"\\nâœ“ Using {len(feature_cols)} raw features for training\")\n",
    "    print(f\"âœ“ Target column: {target_col}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ“ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"âœ“ Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nâœ“ Handling missing values...\")\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with median\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_value = X[col].mode()[0] if not X[col].mode().empty else 'unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\nâœ“ Encoding categorical variables...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  - {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nâœ“ Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    if len(numerical_cols) > 0:\n",
    "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(\"\\nâœ“ Encoding target variable...\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    print(f\"  - Target classes: {target_encoder.classes_}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoders, scaler, target_encoder, feature_cols\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: HANDLE CLASS IMBALANCE WITH CLASS WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_sample_weights(y_train):\n",
    "    \"\"\"Compute class weights to handle imbalance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTraining set class distribution:\")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count:,} samples ({count/len(y_train)*100:.2f}%)\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weights = np.zeros(len(y_train))\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        sample_weights[y_train == cls] = weight\n",
    "    \n",
    "    print(f\"\\nâœ“ Computed class weights:\")\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        print(f\"  Class {cls}: weight = {weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Higher weights give more importance to minority classes\")\n",
    "    \n",
    "    return sample_weights, dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: TRAIN MODELS\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, sample_weights):\n",
    "    \"\"\"Train XGBoost model with validation and class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 1: XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'min_child_weight': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0.1, 1],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"XGBoost Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train Random Forest with class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 2: Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'max_features': ['sqrt'],\n",
    "        'max_samples': [0.8]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Random Forest Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train SVM with class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 3: SVM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svm = SVC(\n",
    "        random_state=42,\n",
    "        probability=True,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SVM Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(y_test, y_pred, target_encoder):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    target_names = target_encoder.classes_\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        n_features = len(feature_names)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"Feature Importance - {model_name}\")\n",
    "        plt.bar(range(n_features), importances[indices])\n",
    "        plt.xticks(range(n_features), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"diabetes_{model_name.lower().replace(' ', '_')}_importance.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Feature Importance Ranking:\")\n",
    "        for i in range(n_features):\n",
    "            print(f\"  {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "        \n",
    "        # Check for dominance\n",
    "        if importances[indices[0]] > 0.5:\n",
    "            print(f\"\\n   âš ï¸  '{feature_names[indices[0]]}' dominates with {importances[indices[0]]*100:.1f}%!\")\n",
    "            print(f\"       The target may be calculated directly from this feature!\")\n",
    "        elif importances[indices[0]] > 0.4:\n",
    "            print(f\"\\n   âš ï¸  '{feature_names[indices[0]]}' has {importances[indices[0]]*100:.1f}% importance\")\n",
    "            print(f\"       This is high - check if it's leaking information\")\n",
    "        else:\n",
    "            print(f\"\\n   âœ“ No single feature dominates - good distribution\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# WellNest ML Training - DIABETES (ULTRA-STRICT VERSION)\")\n",
    "    print(\"# Using ONLY raw clinical measurements\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_diabetes_data(CSV_FILE, TARGET_COLUMN)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Prepare features\n",
    "    result = prepare_features(df, TARGET_COLUMN)\n",
    "    if result[0] is None:\n",
    "        return\n",
    "    \n",
    "    X, y, label_encoders, scaler, target_encoder, feature_names = result\n",
    "    \n",
    "    # Split data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3A: SPLITTING DATA (70-20-10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Validation: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Test: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    sample_weights, class_weights_dict = compute_sample_weights(y_train)\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # XGBoost\n",
    "    models['xgboost'] = train_xgboost(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, sample_weights\n",
    "    )\n",
    "    plot_feature_importance(models['xgboost'], feature_names, 'XGBoost')\n",
    "    \n",
    "    # Random Forest\n",
    "    models['random_forest'] = train_random_forest(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, class_weights_dict\n",
    "    )\n",
    "    plot_feature_importance(models['random_forest'], feature_names, 'RandomForest')\n",
    "    \n",
    "    # SVM\n",
    "    models['svm'] = train_svm(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, class_weights_dict\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: SAVING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        filename = f\"diabetes_{model_name}_raw_{timestamp}.joblib\"\n",
    "        joblib.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'target_encoder': target_encoder,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': TARGET_COLUMN\n",
    "        }, filename)\n",
    "        print(f\"âœ“ Saved: {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ’¡ If you still see >95% accuracy:\")\n",
    "    print(\"   â†’ Your target variable is calculated too simply from raw features\")\n",
    "    print(\"   â†’ Example: IF glucose > 400 THEN urgent (perfect rule)\")\n",
    "    print(\"\\nðŸ’¡ If you see 70-85% accuracy:\")\n",
    "    print(\"   â†’ âœ… GOOD! This is realistic and academically sound\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf424cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "# WellNest ML Training - DIABETES (PATTERN LEARNING VERSION)\n",
      "# Excluding direct glucose/HbA1c measurements\n",
      "# Using categorical/composite features for pattern learning\n",
      "################################################################################\n",
      "================================================================================\n",
      "STEP 1: LOADING DATA\n",
      "================================================================================\n",
      "\n",
      "Reading CSV file: C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv\n",
      "âœ“ Loaded 86,641 rows with 32 columns\n",
      "\n",
      "Final dataset: 86,641 rows\n",
      "\n",
      "Target variable distribution:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            84618\n",
      "needs_attention     1350\n",
      "urgent               673\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportions:\n",
      "GLUCOSE_URGENCY_LEVEL\n",
      "routine            0.976651\n",
      "needs_attention    0.015582\n",
      "urgent             0.007768\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "âš ï¸  WARNING: Severe class imbalance detected!\n",
      "   Smallest class: 0.78%\n",
      "   Will use class weights to handle imbalance\n",
      "\n",
      "================================================================================\n",
      "STEP 2: FEATURE PREPARATION (PATTERN LEARNING)\n",
      "================================================================================\n",
      "\n",
      "ðŸ›¡ï¸ STRATEGIC LEAKAGE PREVENTION:\n",
      "   Excluding direct measurements: BLOOD_GLUCOSE_LEVEL, HBA1C_LEVEL\n",
      "   Including 18 categorical/composite features\n",
      "   This forces the model to learn PATTERNS, not memorize thresholds\n",
      "\n",
      "   âœ“ Allowed features (18):\n",
      "     âœ“ AGE\n",
      "     âœ“ GENDER\n",
      "     âœ“ BMI\n",
      "     âœ“ HAS_HYPERTENSION\n",
      "     âœ“ HAS_HEART_DISEASE\n",
      "     âœ“ SMOKING_HISTORY\n",
      "     âœ“ IS_CURRENT_SMOKER\n",
      "     âœ“ HAS_SMOKING_HISTORY\n",
      "     âœ“ DIABETES_STAGE\n",
      "     âœ“ BMI_CATEGORY\n",
      "     âœ“ CARDIOMETABOLIC_DISEASE_COUNT\n",
      "     âœ“ CARDIOVASCULAR_RISK_SCORE\n",
      "     âœ“ METABOLIC_SYNDROME_SCORE\n",
      "     âœ“ AGE_RISK_CATEGORY\n",
      "     âœ“ HAS_MULTIPLE_CONDITIONS\n",
      "     âœ“ HAS_DIABETES\n",
      "     âœ“ IS_OBESE\n",
      "     âœ“ IS_SEVERELY_OBESE\n",
      "\n",
      "   âŒ Excluded to prevent trivial prediction:\n",
      "     - BLOOD_GLUCOSE_LEVEL\n",
      "     - HBA1C_LEVEL\n",
      "     - HYPERGLYCEMIA_URGENCY\n",
      "     - HYPOGLYCEMIA_URGENCY\n",
      "     - GLUCOSE_CONTROL_STATUS\n",
      "     - GLUCOSE_HBA1C_CONCORDANCE\n",
      "     - DIABETES_COMPLICATION_RISK_SCORE\n",
      "\n",
      "âœ“ Using 18 raw features for training\n",
      "âœ“ Target column: GLUCOSE_URGENCY_LEVEL\n",
      "\n",
      "âœ“ Categorical features: 5\n",
      "âœ“ Numerical features: 5\n",
      "\n",
      "âœ“ Handling missing values...\n",
      "\n",
      "âœ“ Encoding categorical variables...\n",
      "  - GENDER: 3 unique values\n",
      "  - SMOKING_HISTORY: 6 unique values\n",
      "  - DIABETES_STAGE: 4 unique values\n",
      "  - BMI_CATEGORY: 7 unique values\n",
      "  - AGE_RISK_CATEGORY: 3 unique values\n",
      "\n",
      "âœ“ Scaling numerical features...\n",
      "\n",
      "âœ“ Encoding target variable...\n",
      "  - Target classes: ['needs_attention' 'routine' 'urgent']\n",
      "\n",
      "âœ“ Final feature matrix shape: (86641, 18)\n",
      "\n",
      "================================================================================\n",
      "STEP 3A: SPLITTING DATA (70-20-10)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Train: 60,648 (70.0%)\n",
      "âœ“ Validation: 17,337 (20.0%)\n",
      "âœ“ Test: 8,656 (10.0%)\n",
      "\n",
      "================================================================================\n",
      "STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\n",
      "================================================================================\n",
      "\n",
      "Training set class distribution:\n",
      "  Class 0: 945 samples (1.56%)\n",
      "  Class 1: 59,232 samples (97.67%)\n",
      "  Class 2: 471 samples (0.78%)\n",
      "\n",
      "âœ“ Computed class weights:\n",
      "  Class 0: weight = 21.3926\n",
      "  Class 1: weight = 0.3413\n",
      "  Class 2: weight = 42.9214\n",
      "\n",
      "ðŸ’¡ Higher weights give more importance to minority classes\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MODELS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "TRAINING MODEL 1: XGBoost\n",
      "================================================================================\n",
      "\n",
      "â³ Running hyperparameter tuning (with class weights)...\n",
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WellNest Healthcare ML Model Training - DIABETES ONLY (FIXED VERSION)\n",
    "Trains XGBoost, Random Forest, and SVM models for diabetes triage prediction\n",
    "\n",
    "FIXES:\n",
    "- Removed ALL data leakage features\n",
    "- Uses ONLY raw clinical measurements\n",
    "- Added class weights for imbalance\n",
    "- Proper validation and overfitting detection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - UPDATE THIS\n",
    "# =============================================================================\n",
    "\n",
    "# Your CSV file name\n",
    "CSV_FILE = r'C:\\Users\\laksh\\OneDrive\\Desktop\\Sem 4\\GenAI\\Datasets Feature Engineered\\diabetes_feature_engineered.csv'\n",
    "\n",
    "# Your target column name\n",
    "TARGET_COLUMN = 'GLUCOSE_URGENCY_LEVEL'\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD DATA\n",
    "# =============================================================================\n",
    "\n",
    "def load_diabetes_data(csv_file, target_col):\n",
    "    \"\"\"Load diabetes feature data from CSV\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: LOADING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nReading CSV file: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"\\nâŒ ERROR: Target column '{target_col}' not found!\")\n",
    "        print(f\"Available columns: {df.columns.tolist()[:10]}...\")\n",
    "        return None\n",
    "    \n",
    "    # Remove rows with missing target values\n",
    "    initial_rows = len(df)\n",
    "    df = df[df[target_col].notna()]\n",
    "    removed_rows = initial_rows - len(df)\n",
    "    \n",
    "    if removed_rows > 0:\n",
    "        print(f\"âœ“ Removed {removed_rows:,} rows with missing target values\")\n",
    "    \n",
    "    print(f\"\\nFinal dataset: {len(df):,} rows\")\n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f\"\\nTarget proportions:\")\n",
    "    print(df[target_col].value_counts(normalize=True))\n",
    "    \n",
    "    # Warn about class imbalance\n",
    "    class_proportions = df[target_col].value_counts(normalize=True)\n",
    "    min_class_prop = class_proportions.min()\n",
    "    if min_class_prop < 0.1:\n",
    "        print(f\"\\nâš ï¸  WARNING: Severe class imbalance detected!\")\n",
    "        print(f\"   Smallest class: {min_class_prop*100:.2f}%\")\n",
    "        print(f\"   Will use class weights to handle imbalance\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: PREPARE FEATURES (ULTRA-STRICT - RAW ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare features using categorical/composite features\n",
    "    Excludes direct glucose/HbA1c to force pattern learning\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: FEATURE PREPARATION (PATTERN LEARNING)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ðŸš¨ STRATEGIC: Exclude direct measurements that make the task too easy\n",
    "    # Include composite/categorical features that require pattern learning\n",
    "    \n",
    "    allowed_features = [\n",
    "        # Raw demographic/clinical measurements\n",
    "        'AGE',\n",
    "        'GENDER',\n",
    "        'BMI',\n",
    "        'HAS_HYPERTENSION',\n",
    "        'HAS_HEART_DISEASE',\n",
    "        'SMOKING_HISTORY',\n",
    "        'IS_CURRENT_SMOKER',\n",
    "        'HAS_SMOKING_HISTORY',\n",
    "        \n",
    "        # Derived categorical features (require pattern learning, not direct thresholds)\n",
    "        'DIABETES_STAGE',  # Categorizes HbA1c (normal/prediabetes/diabetes)\n",
    "        'BMI_CATEGORY',  # Categorizes BMI (underweight/normal/overweight/obese)\n",
    "        'CARDIOMETABOLIC_DISEASE_COUNT',  # Count of conditions\n",
    "        'CARDIOVASCULAR_RISK_SCORE',  # Composite risk score\n",
    "        'METABOLIC_SYNDROME_SCORE',  # Composite score\n",
    "        'AGE_RISK_CATEGORY',  # Age grouping\n",
    "        'HAS_MULTIPLE_CONDITIONS',  # Flag for multiple diagnoses\n",
    "        'HAS_DIABETES',  # Diagnosis flag\n",
    "        'IS_OBESE',  # Obesity flag\n",
    "        'IS_SEVERELY_OBESE'  # Severe obesity flag\n",
    "    ]\n",
    "    \n",
    "    # EXCLUDE these - they make the task trivial:\n",
    "    excluded_leaky_features = [\n",
    "        'BLOOD_GLUCOSE_LEVEL',  # 88.6% importance - directly predicts target!\n",
    "        'HBA1C_LEVEL',  # Also directly used in target calculation\n",
    "        'HYPERGLYCEMIA_URGENCY',  # Derived FROM target\n",
    "        'HYPOGLYCEMIA_URGENCY',  # Derived FROM target\n",
    "        'GLUCOSE_CONTROL_STATUS',  # Derived FROM target\n",
    "        'GLUCOSE_HBA1C_CONCORDANCE',  # Derived FROM target\n",
    "        'DIABETES_COMPLICATION_RISK_SCORE',  # Might use glucose directly\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nðŸ›¡ï¸ STRATEGIC LEAKAGE PREVENTION:\")\n",
    "    print(f\"   Excluding direct measurements: BLOOD_GLUCOSE_LEVEL, HBA1C_LEVEL\")\n",
    "    print(f\"   Including {len(allowed_features)} categorical/composite features\")\n",
    "    print(f\"   This forces the model to learn PATTERNS, not memorize thresholds\")\n",
    "    \n",
    "    print(f\"\\n   âœ“ Allowed features ({len(allowed_features)}):\")\n",
    "    for feat in allowed_features:\n",
    "        if feat in df.columns:\n",
    "            print(f\"     âœ“ {feat}\")\n",
    "        else:\n",
    "            print(f\"     âœ— {feat} (not found in data)\")\n",
    "    \n",
    "    print(f\"\\n   âŒ Excluded to prevent trivial prediction:\")\n",
    "    for feat in excluded_leaky_features:\n",
    "        if feat in df.columns:\n",
    "            print(f\"     - {feat}\")\n",
    "    \n",
    "    # Get feature columns (only allowed features that exist)\n",
    "    feature_cols = [col for col in allowed_features if col in df.columns]\n",
    "    \n",
    "    if len(feature_cols) == 0:\n",
    "        print(\"\\nâŒ ERROR: No valid features found!\")\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    print(f\"\\nâœ“ Using {len(feature_cols)} raw features for training\")\n",
    "    print(f\"âœ“ Target column: {target_col}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col].copy()\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ“ Categorical features: {len(categorical_cols)}\")\n",
    "    print(f\"âœ“ Numerical features: {len(numerical_cols)}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"\\nâœ“ Handling missing values...\")\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with median\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        missing_count = X[col].isnull().sum()\n",
    "        if missing_count > 0:\n",
    "            mode_value = X[col].mode()[0] if not X[col].mode().empty else 'unknown'\n",
    "            X[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  - {col}: filled {missing_count} missing values with '{mode_value}'\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    print(\"\\nâœ“ Encoding categorical variables...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  - {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Scale numerical features\n",
    "    print(\"\\nâœ“ Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    if len(numerical_cols) > 0:\n",
    "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(\"\\nâœ“ Encoding target variable...\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y)\n",
    "    print(f\"  - Target classes: {target_encoder.classes_}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X, y_encoded, label_encoders, scaler, target_encoder, feature_cols\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: HANDLE CLASS IMBALANCE WITH CLASS WEIGHTS\n",
    "# =============================================================================\n",
    "\n",
    "def compute_sample_weights(y_train):\n",
    "    \"\"\"Compute class weights to handle imbalance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3B: HANDLING CLASS IMBALANCE WITH CLASS WEIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nTraining set class distribution:\")\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count:,} samples ({count/len(y_train)*100:.2f}%)\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    # Create sample weights\n",
    "    sample_weights = np.zeros(len(y_train))\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        sample_weights[y_train == cls] = weight\n",
    "    \n",
    "    print(f\"\\nâœ“ Computed class weights:\")\n",
    "    for cls, weight in zip(np.unique(y_train), class_weights):\n",
    "        print(f\"  Class {cls}: weight = {weight:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Higher weights give more importance to minority classes\")\n",
    "    \n",
    "    return sample_weights, dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: TRAIN MODELS\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, sample_weights):\n",
    "    \"\"\"Train XGBoost model with validation and class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 1: XGBoost\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'min_child_weight': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'reg_alpha': [0.1, 1],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb, \n",
    "        param_grid, \n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate on all sets\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"XGBoost Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train Random Forest with class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 2: Random Forest\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 15],\n",
    "        'min_samples_split': [5, 10],\n",
    "        'min_samples_leaf': [2, 4],\n",
    "        'max_features': ['sqrt'],\n",
    "        'max_samples': [0.8]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Random Forest Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_svm(X_train, y_train, X_val, y_val, X_test, y_test, target_encoder, class_weights_dict):\n",
    "    \"\"\"Train SVM with class weights\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING MODEL 3: SVM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "    svm = SVC(\n",
    "        random_state=42,\n",
    "        probability=True,\n",
    "        class_weight=class_weights_dict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâ³ Running hyperparameter tuning (with class weights)...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        svm,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\nâœ“ Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"âœ“ Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SVM Results:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"\\nðŸ“Š TRAIN Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    val_acc = accuracy_score(y_val, y_val_pred)\n",
    "    print(f\"ðŸ“Š VALIDATION Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"ðŸ“Š TEST Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    train_val_gap = train_acc - val_acc\n",
    "    print(f\"\\nðŸ” Train-Val gap: {train_val_gap:.4f}\")\n",
    "    \n",
    "    if train_acc > 0.95 and val_acc > 0.95:\n",
    "        print(\"   âš ï¸  BOTH train and val are >95% - STILL DATA LEAKAGE!\")\n",
    "    elif train_val_gap > 0.10:\n",
    "        print(\"   âš ï¸  Large gap - overfitting\")\n",
    "    else:\n",
    "        print(\"   âœ“ Reasonable performance\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TEST Set Classification Report:\")\n",
    "    print(\"-\"*80)\n",
    "    evaluate_model(y_test, y_test_pred, target_encoder)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(y_test, y_pred, target_encoder):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    target_names = target_encoder.classes_\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "def plot_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Plot feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        n_features = len(feature_names)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(f\"Feature Importance - {model_name}\")\n",
    "        plt.bar(range(n_features), importances[indices])\n",
    "        plt.xticks(range(n_features), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Importance\")\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"diabetes_{model_name.lower().replace(' ', '_')}_importance.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nâœ“ Saved: {filename}\")\n",
    "        \n",
    "        print(f\"\\nâœ“ Feature Importance Ranking:\")\n",
    "        for i in range(n_features):\n",
    "            print(f\"  {i+1}. {feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "        \n",
    "        # Check for dominance\n",
    "        if importances[indices[0]] > 0.5:\n",
    "            print(f\"\\n   âš ï¸  '{feature_names[indices[0]]}' dominates with {importances[indices[0]]*100:.1f}%!\")\n",
    "            print(f\"       The target may be calculated directly from this feature!\")\n",
    "        elif importances[indices[0]] > 0.4:\n",
    "            print(f\"\\n   âš ï¸  '{feature_names[indices[0]]}' has {importances[indices[0]]*100:.1f}% importance\")\n",
    "            print(f\"       This is high - check if it's leaking information\")\n",
    "        else:\n",
    "            print(f\"\\n   âœ“ No single feature dominates - good distribution\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# WellNest ML Training - DIABETES (PATTERN LEARNING VERSION)\")\n",
    "    print(\"# Excluding direct glucose/HbA1c measurements\")\n",
    "    print(\"# Using categorical/composite features for pattern learning\")\n",
    "    print(\"#\"*80)\n",
    "    \n",
    "    # Load data\n",
    "    df = load_diabetes_data(CSV_FILE, TARGET_COLUMN)\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Prepare features\n",
    "    result = prepare_features(df, TARGET_COLUMN)\n",
    "    if result[0] is None:\n",
    "        return\n",
    "    \n",
    "    X, y, label_encoders, scaler, target_encoder, feature_names = result\n",
    "    \n",
    "    # Split data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3A: SPLITTING DATA (70-20-10)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.333, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Validation: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"âœ“ Test: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    sample_weights, class_weights_dict = compute_sample_weights(y_train)\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # XGBoost\n",
    "    models['xgboost'] = train_xgboost(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, sample_weights\n",
    "    )\n",
    "    plot_feature_importance(models['xgboost'], feature_names, 'XGBoost')\n",
    "    \n",
    "    # Random Forest\n",
    "    models['random_forest'] = train_random_forest(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, class_weights_dict\n",
    "    )\n",
    "    plot_feature_importance(models['random_forest'], feature_names, 'RandomForest')\n",
    "    \n",
    "    # SVM\n",
    "    models['svm'] = train_svm(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "        target_encoder, class_weights_dict\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 5: SAVING MODELS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        filename = f\"diabetes_{model_name}_raw_{timestamp}.joblib\"\n",
    "        joblib.dump({\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'label_encoders': label_encoders,\n",
    "            'target_encoder': target_encoder,\n",
    "            'feature_names': feature_names,\n",
    "            'target_column': TARGET_COLUMN\n",
    "        }, filename)\n",
    "        print(f\"âœ“ Saved: {filename}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nâœ“ If you still see >90% accuracy:\")\n",
    "    print(\"   â†’ Check feature importance - another feature might be leaking\")\n",
    "    print(\"\\nâœ“ If you see 70-85% accuracy:\")\n",
    "    print(\"   â†’ âœ… PERFECT! Model is learning patterns, not memorizing rules\")\n",
    "    print(\"   â†’ This is academically rigorous and publishable\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
